{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question_Generation_with_Transformer.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"192vSaYza2L_S29b-cfeTZ06sapmf_ZDg","authorship_tag":"ABX9TyOZ6SNPgDa+OnjaoswiOES/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ylgLZ1wDlxvT"},"source":["### Configure Dependencies"]},{"cell_type":"code","metadata":{"id":"yYwS_xXUljlt","executionInfo":{"status":"ok","timestamp":1603511879717,"user_tz":240,"elapsed":2859,"user":{"displayName":"Hao Hao","photoUrl":"","userId":"02688432240746996757"}},"outputId":"b488bcb6-75b6-4f6b-ff0f-5348e9f3dc6f","colab":{"base_uri":"https://localhost:8080/","height":427}},"source":["import os\n","import sys\n","import math\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import spacy\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install transformers\n","from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tyg8-Dd3mEVf","executionInfo":{"status":"ok","timestamp":1603511879718,"user_tz":240,"elapsed":2845,"user":{"displayName":"Hao Hao","photoUrl":"","userId":"02688432240746996757"}},"outputId":"bdb753b1-1d2d-46d4-efb6-28ea621c3a67","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Using device: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eB9cJdDdn4zU"},"source":["### Dataloaders"]},{"cell_type":"code","metadata":{"id":"gQtYF9IcmL8r","executionInfo":{"status":"ok","timestamp":1603511881073,"user_tz":240,"elapsed":4193,"user":{"displayName":"Hao Hao","photoUrl":"","userId":"02688432240746996757"}},"outputId":"84903e62-8156-49e7-8f26-4cb78fc77c66","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","PRETRAINED_MODEL = 't5-base'\n","BATCH_SIZE = 4\n","SEQ_LENGTH = 512\n","\n","tokenizer = T5Tokenizer.from_pretrained(PRETRAINED_MODEL)\n","tokenizer.add_special_tokens(\n","    {'additional_special_tokens': ['<answer>', '<context>']}\n",")\n","\n","class QGDataset(Dataset):\n","    def __init__(self, csv):\n","        self.df = pd.read_csv( csv, engine='python')\n","\n","    def __len__(self):\n","         return len(self.df)\n","\n","    def __getitem__(self, idx):   \n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        row = self.df.iloc[idx, 1:]       \n","\n","        encoded_text = tokenizer(\n","            row['text'], \n","            pad_to_max_length=True, \n","            max_length=SEQ_LENGTH,\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","        encoded_text['input_ids'] = torch.squeeze(encoded_text['input_ids'])\n","        encoded_text['attention_mask'] = torch.squeeze(encoded_text['attention_mask'])\n","\n","        encoded_question = tokenizer(\n","            row['question'],\n","            pad_to_max_length=True,\n","            max_length=SEQ_LENGTH,\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        encoded_question['input_ids'] = torch.squeeze(encoded_question['input_ids'])\n","\n","        return (encoded_text.to(device), encoded_question.to(device))\n","\n","train_set = QGDataset('/content/drive/My Drive/NLP_QA_Project/Data/train.csv')\n","train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n","valid_set = QGDataset('/content/drive/My Drive/NLP_QA_Project/Data/val.csv') \n","valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)\n","print('dataloader DONE')"],"execution_count":32,"outputs":[{"output_type":"stream","text":["dataloader DONE\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GUMpGn34oTp3"},"source":["### Model"]},{"cell_type":"code","metadata":{"id":"L05aCctBn2X5","executionInfo":{"status":"ok","timestamp":1603511891594,"user_tz":240,"elapsed":14705,"user":{"displayName":"Hao Hao","photoUrl":"","userId":"02688432240746996757"}}},"source":["LR = 0.001\n","EPOCHS = 20\n","LOG_INTERVAL = 5000\n","\n","config = T5Config(decoder_start_token_id=tokenizer.pad_token_id)\n","model = T5ForConditionalGeneration(config).from_pretrained(PRETRAINED_MODEL)\n","model.resize_token_embeddings(len(tokenizer)) # to account for new special tokens\n","model = model.to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tPYvAv_0oj3c"},"source":["### Train and Evaluate"]},{"cell_type":"code","metadata":{"id":"-boJBhAtol96","executionInfo":{"status":"ok","timestamp":1603511891596,"user_tz":240,"elapsed":14703,"user":{"displayName":"Hao Hao","photoUrl":"","userId":"02688432240746996757"}}},"source":["\n","SAVED_MODEL_PATH = \"/content/drive/My Drive/NLP_QA_Project/model/qg_pretrained_t5_model_fine_tuned.pth\"\n","\n","def train(epoch, best_val_loss):\n","    model.train()\n","    total_loss = 0.\n","    for batch_index, batch in enumerate(train_loader):\n","        data, target = batch\n","        optimizer.zero_grad()\n","        masked_labels = mask_label_padding(target['input_ids'])\n","        output = model(\n","            input_ids=data['input_ids'],\n","            attention_mask=data['attention_mask'],\n","            lm_labels=masked_labels\n","        )\n","        loss = output[0]\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        if batch_index % LOG_INTERVAL == 0 and batch_index > 0:\n","            cur_loss = total_loss / LOG_INTERVAL\n","            print('| epoch {:3d} | ' \n","                  '{:5d}/{:5d} batches | '\n","                  'loss {:5.2f}'.format(\n","                    epoch, \n","                    batch_index, len(train_loader), \n","                    cur_loss))\n","            save(\n","                TEMP_SAVE_PATH,\n","                epoch, \n","                model.state_dict(), \n","                optimizer.state_dict(), \n","                best_val_loss\n","            )\n","            total_loss = 0\n","\n","def evaluate(eval_model, data_loader):\n","    eval_model.eval()\n","    total_loss = 0.\n","    with torch.no_grad():\n","        for batch_index, batch in enumerate(data_loader):\n","            data, target = batch\n","            masked_labels = mask_label_padding(target['input_ids'])\n","            output = eval_model(\n","                input_ids=data['input_ids'],\n","                attention_mask=data['attention_mask'],\n","                lm_labels=masked_labels\n","            )\n","            total_loss += output[0].item()\n","    return total_loss / len(data_loader)\n","\n","def mask_label_padding(labels):\n","    MASK_ID = -100\n","    labels[labels==tokenizer.pad_token_id] = MASK_ID\n","    return labels\n","\n","def save(path, epoch, model_state_dict, optimizer_state_dict, loss):\n","    torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model_state_dict,\n","            'optimizer_state_dict': optimizer_state_dict,\n","            'best_loss': loss,\n","            }, path)\n","\n","def load(path):\n","    return torch.load(path)\n","\n","def print_line():\n","    LINE_WIDTH = 60\n","    print('-' * LINE_WIDTH)"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AF6JyS0Loymr"},"source":["### Routine"]},{"cell_type":"code","metadata":{"id":"8aqIJhzzoyvu","outputId":"f3e611ff-f787-46d4-de9f-56c10a17e12d","colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["best_val_loss = float(\"inf\")\n","best_model = None\n","\n","val_loss = evaluate(model, valid_loader)\n","print_line()\n","print('| Before training | valid loss {:5.2f}'.format(\n","    val_loss)\n",")\n","print_line()\n","\n","for epoch in range(1, EPOCHS + 1):\n","\n","    train()\n","    val_loss = evaluate(model, valid_loader)\n","    print_line()\n","    print('| end of epoch {:3d} | valid loss {:5.2f}'.format(\n","        epoch,\n","        val_loss)\n","    )\n","    print_line()\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_model = model\n","        save(\n","             SAVED_MODEL_PATH,\n","             epoch, \n","             model.state_dict(), \n","             optimizer.state_dict(), \n","             best_val_loss\n","        )\n","        print(\"| Model saved.\")\n","        print_line()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1146: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n","  FutureWarning,\n"],"name":"stderr"}]}]}